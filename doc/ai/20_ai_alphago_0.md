# AlphaGo原理

1、引入策略价值神经网络，这个策略价值网络实际包括两张人工神经网络：
* 策略网络（Policy Network），也称为走棋网络，给定当前局面，预测/采样下一步的走棋；传统的MCTS，expand树节点时候，采用简单的采用平均策略
* 估值网络（Value Network），给定当前局面，估计是白胜还是黑胜；传统的MCTS通过随机模拟对局，估算当前局面（扩展的子节点后的局面）的value

2、通过强化学习，也即是通过自我对弈进行神经网络的训练

## 自我对局（self-play）
在self-play过程中，我们会收集一系列的(s,pi,z)数据，s表示局面，pi是根据MCTS根节点处每个分支的访问次数计算的概率, z是self-play对局的结果，其中 pi 和 z 需要特别注意从每一步的当前player的视角去表示.

 ![alphago_selfplay](./img/alphago_selfplay.jpg)

## 策略价值网络训练
输入: 局面s
策略网络输出: pi每一个可行action的概率
价值网络输出: z当前局面评分的模型

策略价值网络，包括两张网络，一张是走棋网络（策略网络），一张是价值网络；
就是在给定当前局面s的情况下，返回当前局面下每一个可行action的概率以及当前局面评分的模型。前面self-play收集到的数据就是用来训练策略价值网络的，而训练更新的策略价值网络也会马上被应用到MCTS中进行后面的self-play，以生成更优质的self-play数据。两者相互嵌套，相互促进，就构成了整个训练的循环

 ![alphago_train](./img/alphago_train.jpg)

1. 局面描述方式

在AlphaGo Zero中，一共使用了17个s的二值特征平面来描述当前局面，其中前16个平面描述了最近8步对应的双方player的棋子位置，最后一个平面描述当前player对应的棋子颜色，其实也就是先后手。

在我们的实现中，对局面的描述进行了极大的简化，以8*8的棋盘为例，我们只使用了4个8*8的二值特征平面：
1.1、前两个平面分别表示当前player的棋子位置和对手player的棋子位置，有棋子的位置是1，没棋子的位置是0.
1.2、第三个平面表示对手player最近一步的落子位置，也就是整个平面只有一个位置是1，其余全部是0.
1.3、第四个平面，也就是最后一个平面表示的是当前player是不是先手player，如果是先手player则整个平面全部为1，否则全部为0.

在最开始尝试的时候，我只用了前两个平面，也就是双方的棋子的位置，因为直观感觉这两个平面已经足够表达整个完整的局面了。但是后来在增加了后两个特征平面之后，训练的效果有了比较明显的改善。个人猜想，因为在五子棋中，我方下一步的落子位置往往会在对手前一步落子位置的附近，所以加入的第三个平面对于策略网络确定哪些位置应该具有更高的落子概率具有比较大的指示意义，可能有助有训练。同时，因为先手在对弈中其实是很占优势的，所以在局面上棋子位置相似的情况下，当前局面的优劣和当前player到底是先手还是后手十分相关，所以第四个指示先后手的平面可能对于价值网络具有比较大的意义。

2. 网络结构
在AlphaGo Zero中，输入局面首先通过了20或40个基于卷积的残差网络模块，然后再分别接上2层或3层网络得到策略和价值输出，整个网络的层数有40多或80多层，训练和预测的时候都十分缓慢。

所以在我们的实现中，对这个网络结构进行了极大的简化，最开始是公共的3层全卷积网络，分别使用32、64和128个3*3的filter，使用ReLu激活函数。然后再分成policy和value两个输出，在policy这一端，先使用4个1*1的filter进行降维，再接一个全连接层，使用softmax非线性函数直接输出棋盘上每个位置的落子概率；在value这一端，先使用2个1*1的filter进行降维，再接一个64个神经元的全连接层，最后再接一个全连接层，使用tanh非线性函数直接输出[-1,1]之间的局面评分。整个策略价值网络的深度只有5~6层，训练和预测都相对比较快。

3.训练目标
前面提到，策略价值网络的输入是当前的局面描述s，输出是当前局面下每一个可行action的概率p 以及当前局面的评分v ，而用来训练策略价值网络的是我们在self-play过程中收集的一系列的(s,pi,z)  数据。根据上面的策略价值网络训练示意图，我们训练的目标是让策略价值网络输出的action概率  更加接近MCTS输出的概率pi,让策略价值网络输出的局面评分v能更准确的预测真实的对局结果z  。从优化的角度来说，我们是在self-play数据集上不断的最小化损失函数，

损失函数包括两个部分：

* 策略网络的损失loss_pi，采用交叉熵误差作为损失函数：loss_pi  = -Σpi * log(p)
* 价值网络的损失loss_z，采用平方误差作为损失函数：loss_z = (z-v)^2

loss = loss_pi + loss_z

